\documentclass[a4paper,11pt]{scrartcl}

\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE}
% \VignetteIndexEntry{Examples on using StatMatch to integrate two data sources}

\usepackage[OT1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor=blue, linkcolor=blue, urlcolor=blue}
%\usepackage[top=35mm, bottom=30mm, left=35mm, right=30mm]{geometry}

%% additional commands
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\mbox{\textbf{#1}}}
\newcommand{\proglang}[1]{\mbox{\textsf{#1}}}
\newcommand{\dQuote}[1]{``#1''}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Statistical Matching and Imputation of Survey Data with the Package 
        \pkg{StatMatch} for the \proglang{R} Environment\footnote{This work was developed in the framework of the ESSnet project on Data Integration, partly funded by Eurostat (December 2009--December 2011).  For more information on the project visit \url{http://www.essnet-portal.eu/di/data-integration}}
}
\author{
  Marcello D'Orazio\\
   \large \emph{Italian National Institute of Statistics (Istat), Rome, Italy}\\
   \large E-mail: \href{mailto:madorazi@istat.it}{madorazi@istat.it}
}

\date{\large October 19, 2011}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle

%\paragraph{Abstract}
%In this paper, it is shown hot to use the functions in the \proglang{R} package \pkg{StatMatch} in order to integrate different data %sources using some statistical matching techniques.  Moreover some functions in \pkg{StatMatch} can be used to impute missing values %in survey data.  Different approaches to Statistical matching will be shown by using some artificial data.


% ------------
% introduction
% ------------

\section{Introduction} \label{sec:intro}

Statistical matching techniques aim at integrating two or more data sources (usually data from sample surveys) referred to the same target population.  In the basic statistical matching  framework, there are two data sources $A$ and $B$ sharing a set of variables $X$ then the variable $Y$ is available only in $A$ while the variable $Z$ is observed just in $B$.  In practice the $X$ variables are common to both the data sources, while the variables $Y$ and $Z$ are not jointly observed.  The objective of statistical matching (hereafter denoted as SM) consists in investigating the relationship between $Y$ and $Z$.  This objective can be achieved through a \dQuote{micro} or a \dQuote{macro} approach (D'Orazio \emph{et al.}, 2006b).  In the micro approach SM aims at creating a \dQuote{synthetic} data source in which all the variables, $X$, $Y$ and $Z$, are available (usually $A$ filled in with the values of $Z)$.  In the macro approach the data sources are used to derive an estimate of the parameter of interest, e.g. the correlation coefficient between $Y$ and $Z$ or the contingency table $Y \times Z$.  SM can be performed in a parametric or in a nonparametric framework.  The parametric approach requires the explicit adoption of a model for $(X,Y,Z)$; obviously if the model is misspecified then the results will not be reliable.  The nonparametric approach is more flexible in handling complex situations (mixed type variables).  The two approaches can be mixed: first a parametric model is assumed and its parameters are estimated then a completed synthetic data set is derived through a nonparametric micro approach.  In this manner the advantages of both parametric and nonparametric approaches are maintained: the model is parsimonious while nonparametric techniques offer protection against model misspecification.  An interesting comparison of some mixed methods that deal with continuous $X$, $Y$ and $Z$ variables is carried out by (Moriarity and Scheuren, 2001, 2003).  A further comparison is available in (D'Orazio \emph{et al.}, 2005).  Table \ref{SMapproaches} provides a summary of the objectives and approaches to SM (D'Orazio \emph{et al.}, 2008).

\begin{table}
\begin{center}
\caption{Objectives and approaches to Statistical matching.}
\begin{tabular}{c|ccc}
 \hline
 Objectives of & \multicolumn{3}{c}{Approaches to statistical Matching} \\
 Statistical matching & Parametric & Nonparametric & Mixed \\
\hline
  MAcro & yes & yes & no \\
  MIcro & yes & yes & yes \\
\hline
\end{tabular}
\label{SMapproaches}
\end{center}
\end{table}

It is worth noting that in the traditional SM framework when only $A$ and $B$ are available, all the SM methods (parametric, nonparametric and mixed) that use the set of common variables $X$ to match $A$ and $B$, implicitly assume the \emph{conditional independence} (CI) of $Y$ and $Z$ given $X$:

$$ 
f\left( x,y,z \right)=f \left( y|x \right) \times f\left( z|x \right) \times f\left( x \right)
$$

This assumption is particularly strong and seldom holds in practice.  In order to avoid the CI assumption the SM should incorporate some auxiliary information concerning the relationship between $Y$ and $Z$ (see Chap. 3 in D'Orazio \emph{et al.} 2006b).  The auxiliary information can be at micro level (a new data source in which $Y$ and $Z$ or $X$, $Y$ and $Z$ are jointly observed) or at macro level (e.g. an estimate of the correlation coefficient $\rho_{XY}$ or an estimate of the contingency table $Y \times Z$, etc.) or simply consist of some logic constraints about the relationship between $Y$ and $Z$ (structural zeros, etc.; for further details see D'Orazio \emph{et al.}, 2006a).

An alternative approach to SM consists in evaluating the \emph{uncertainty} concerning an estimate of the parameter of interest.  This uncertainty is due to the lack of joint information concerning $Y$ and $Z$.  For instance, let us consider a SM application whose target consists in estimating the correlation matrix of the trivariate normal distribution holding for $(X,Y,Z)$; in the basic SM framework the available data allow to estimate all the components of the correlation matrix with the exception of $\rho_{YZ}$; in this case, due to the properties of the correlation matrix (has to be semidefinite positive), it is possible to conclude that:

$$
\rho_{XY} \rho_{XZ} - \sqrt{\left( 1 - \rho_{YX}^2\right) \left( 1 - \rho_{XZ}^2\right)} 
\leq \rho_{YZ} \leq 
\rho_{XY} \rho_{XZ} + \sqrt{\left( 1 - \rho_{YX}^2\right) \left( 1 - \rho_{XZ}^2\right)}
$$

The higher is the correlation between $X$ and $Y$ and between $X$ and $Z$, the shorter will be the interval and consequently the lower will be the uncertainty.  In practical applications, by substituting the unknown correlation coefficient with the corresponding estimates it is possible to derive a \dQuote{range} of admissible values of the unknown $\rho_{YZ}$.  The topic of the uncertainty will be covered in the Section \ref{sec:unc}.

Section \ref{sec:SMapplication} will be discuss some practical aspects concerning the preliminary steps before applying SM techniques, with a particular emphasis on the choice of the marching variables;  moreover some example data will be introduced.  In Section \ref{sec:npmicro} some approaches to SM at micro level based on nonparametric methods will be shown. Section \ref{sec:mix} is devoted to the mixed approaches to SM when dealing with continuous variables.  Section \ref{sec:comp_survey} will discuss approaches to SM when dealing with data arising from complex sample surveys from finite populations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Application of SM

\section{Practical steps in an application of statistical matching} \label{sec:SMapplication}

Before applying SM methods in order to integrate two or more data sources some decisions and preprocessing steps are required (see Scanu, 2008).  In practice, given two data sources $A$ and $B$ the following steps are necessary:

\begin{itemize}
	\item Choice of the target variables $Y$ and $Z$, i.e. of the variables observed distinctly in two sample surveys.
	\item Identification of all the common variables $X$ shared by $A$ and $B$.  In this step some harmonization procedures may be required because of different definitions and/or classifications.  Obviously, if two similar variables can not be harmonized they have to be discarded.  The common variables should not present missing values and the observed values should be accurate (low or absent measurement error).  It is worth noting that the common variable in the two data sources should have the same marginal/joint distribution, if $A$ and $B$ are representative samples of the same population.
	\item Potentially all the $X$ variables can be used as matching variables but actually, not all them are used in the SM.  Section \ref{sec:mtc_vars} will provide more details concerning this topic.
	\item The choice of the matching variables is strictly related to the choice concerning the matching framework, i.e. micro or macro objective, parametric, nonparametric or a mixed approach etc.
	\item Once decided the framework, a SM technique is used to match the samples.
	\item Finally the results of the matching, whereas possible, should be evaluated.
	
\end{itemize}

%%%%%%%%%%%%%%%%%
% Example data

\subsection{Example data} \label{sec:data}
The next Sections will provide a series of examples of application of some SM techniques in the \proglang{R} environment (R Development Core Team, 2011) by using the functions provided by the package \pkg{StatMatch} (D'Orazio, 2011).  These examples will refer to artificial data derived from the data set \code{eusilcS} contained in the package \pkg{simPopulation} (Alfons and Kraft,2011).  This is an artificial data set generated from real Austrian EU-SILC (European Union Statistics on Income and Living Conditions) data containing $11\,725$ observations on 18 variables (see \code{eusilcS} help pages for details):

% change some options
<<echo=FALSE, results=hide>>=
options(useFancyQuotes="UTF-8")
#options(useFancyQuotes=FALSE)
options(width=65)
options(warn=-1)
@


<<>>=
library(simPopulation) #loads pkg simPopulation
data(eusilcS) 
str(eusilcS)
@

In order to use these data to show how SM works, some manipulations are needed to discard units not relevant (obs. with \code{age<16}, whose income and personal economic status are missing), to categorize some variables, etc.

<<>>=
# discard units with age<16
silc.16 <- subset(eusilcS, age>15) # units 
nrow(silc.16)
#
# categorize age
silc.16$c.age <- cut(silc.16$age, c(16,24,49,64,100), include.lowest=T)
#
# truncate hsize
aa <- as.numeric(silc.16$hsize)
aa[aa>6] <- 6
silc.16$hsize6 <- factor(aa, ordered=T)
#
# recode personal economic status
aa <- as.numeric(silc.16$pl030)
aa[aa<3] <- 1
aa[aa>1] <- 2
silc.16$work <- factor(aa, levels=1:2, labels=c("working","not working"))
#
# categorize personal net income
silc.16$c.netI <- cut(silc.16$net/1000,
                      breaks=c(-6,0,5,10,15,20,25,30,40,50,200))
@

In order to reproduce the basic SM framework, the data frame \code{silc.16} is split randomly in two data sets: \code{rec.A} consisting of $4\,000$ observations and \code{don.B} with the remaining $5\,522$ units.  The two data frames \code{rec.A} and \code{don.B} share the variables \code{X.vars}; the person's economic status (\code{y.var}) is available only in \code{rec.A} while the net income (\code{z.var}) is available in \code{don.B}.


<<>>=
set.seed(123456)
obs.A <- sample(nrow(silc.16), 4000, replace=F)
X.vars <- c("hsize","hsize6","db040","age","c.age",
            "rb090","pb220a","rb050")
y.var <- c("pl030","work")
z.var <- c("netIncome", "c.netI")
rec.A <- silc.16[obs.A, c(X.vars, y.var)]
don.B <- silc.16[-obs.A, c(X.vars, z.var)]
#
# determine a rough weighting 
# compute N, the est. size of pop(age>16)
N <- round(sum(silc.16$rb050)) 
N
#rescale origin weights
rec.A$wwA <- rec.A$rb050/sum(rec.A$rb050)*N
don.B$wwB <- don.B$rb050/sum(don.B$rb050)*N
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The choice of the matching variables} \label{sec:mtc_vars}

In SM $A$ and $B$, may share many common variables.  In practice, not all the common variables are used in SM but just the most relevant ones.  The selection of the common variables to be used when performing SM, usually called \emph{matching variables}, should be performed through opportune statistical methods (descriptive, inferential, etc.) and by consulting subject matter experts.

From a statistical point of view, the choice of the marching variables $X_M$ $(X_M \subseteq X)$ should be carried out in a \dQuote{multivariate sense} in order to identify the subset of the $X_M$ variables connected at the same time with $Y$ and $Z$ (Cohen, 1991); unfortunately this would require the availability of an auxiliary data source in which all the variables $(X,Y,Z)$ are observed.  In the basic SM framework the data in $A$ permit to explore the relationship between $Y$ and $X$, while the relationship between $Z$ and $X$ can be investigated in the file $B$.  Then the results of the two separate analyses have to be combined in some manner; usually the subset of the matching variables is obtained as $X_M = X_Y \cup X_Z $ , being $X_Y$ $(X_Y \subseteq X)$ the subset of the common variables that better explains $Y$, while $X_Z$ is the subset of the common variables that better explain $Z$ $(X_Z \subseteq X)$.

The simplest procedure to identify $X_Y$ consists in fitting regression models to data available in $A$.  The same procedure is carried out with data in $B$ in order to identify the subset of the common variables $X_Z$ that better explains $Z$.

The following \proglang{R} examples provide an idea of how to proceed.  In particular, in the example data, the variable $Y$ in $A$ is a categorical binary variable (\code{work}), therefore a logistic regression model is fitted in order to identify $X_Y$.

<<>>=
# analyses on A
# logistic regression
work.glm <- glm(work~hsize+db040+age+rb090+pb220a, data=rec.A, 
                family=binomial(link = "logit"))
summary(work.glm)
@

In order to identify $X_Y$ an automatic selection procedure can be used (forward, backward or stepwise selection):

<<>>=
# stepwise selection
new.work.glm <- step(work.glm, trace=0)
summary(new.work.glm)

# X_Y
X.Y.glm <- c("age","rb090", "pb220a")
@

According to the automatic selection procedure it comes out that $X_Y$ is composed by three variables.

In complex situations when a nonlinear relationship is supposed to exist, the selection of the subset $X_Y$ can be demanded to nonparametric procedures such as \emph{Classification And Regression Trees} (Breiman \emph{et al.}, 1984).  Instead of fitting a single tree it may be better to fit a \emph{random forest} (Breiman, 2001).  The random forest can be fitted by using the function \code{randomForest} available in the package \pkg{randomForest} (Liaw and Wiener, 2002).  This technique provides a measure of importance for the predictors (that has to be used with caution).  Alternatively conditional inference trees (Hothorn \emph{et al.}, 2006) can be fitted; in this case the tree fitting is based on hypothesis testing and by considering explicitly the association between responses and covariates.  In \proglang{R} these procedures are made available by the functions in the package \pkg{party} (Hothorn \emph{et al.}, 2006).

The same analysis carried out in $A$ should be carried out in $B$.  In the example data, where the target variable $Z$ in $B$ is the \code{netIncome} (better to consider its log).

<<>>=
summary(don.B$netIncome)
don.B$lognetI <- log(don.B$netIncome+4373+1)
summary(don.B$lognetI)

#regression
lnetI.lm <- lm(lognetI~hsize+db040+age+rb090+pb220a, data=don.B)
summary(lnetI.lm)

# stepwise selection
new.lnetI.lm <- step(lnetI.lm, trace=0)
summary(new.lnetI.lm)

# X_Z
X.Z.lm <- c("hsize","db040","age","rb090", "pb220a")
@

This regression analysis ends with a model including all the available predictors.  Then, according to the results of the two separate analyses on the example data, if it is decided that the matching variables are obtained as $X_M = X_Y \cup X_Z $, it comes out that all the available common variables should be used in the SM application.

<<>>=
# the matching variables
union(X.Y.glm, X.Z.lm)
@

If a smaller subset of the matching variables is necessary, then by considering $X_M=X_Y \cap X_Z$, it would result a subset consisting of just three $X$ variables:

<<>>=
#the smallest subset of matching variables
intersect(X.Y.glm, X.Z.lm)
@

The approach to SM based on the study of uncertainty offers the possibility of choosing the matching variable in a better way: by selecting just those common variables with the highest contribution to the reduction of the uncertainty.  The function \code{Fbwidths.by.x} in \pkg{StatMatch} permits to explore the reduction of uncertainty when all the variables $(X,Y,Z)$ are categorical.  In particular, in the basic SM framework it is possible to show that 

$$
P^{(low)}_{j,k} \leq P(Y=j,Z=k) \leq P^{(up)}_{j,k}, \ \ j=1,2,\ldots,J; \ \ k=1,2,\ldots,K
$$

\noindent being

\begin{eqnarray}
P^{(low)}_{j,k} &=& \sum_{i}  P(X=i) \max \left\{0; P(Y=j|X=i) + P(Z=k|X=i)-1 \right\} \nonumber\\
P^{(up)}_{j,k} &=& \sum_{i}  P(X=i) \min \left\{P(Y=j|X=i); P(Z=k|X=i)\right\} \nonumber
\end{eqnarray}

\noindent for $j=1,\ldots, J$ and $k=1,\ldots, K$, being $J$ and $K$ the categories of $Y$ and $Z$ respectively.

The function \code{Fbwidths.by.x} estimates $(P^{(low)}_{j,k},P^{(up)}_{j,k})$ for each cell in the contingency table $Y \times Z$ in correspondence of all the possible combinations of the $X$ variables; then the reduction of uncertainty is measured naively by considering the average widths of the intervals:

$$
\bar{d} = \frac{1}{J \times K} \sum_{j=1}^{J} \sum_{k=1}^{K} ( \hat{P}^{(up)}_{j,k} - \hat{P}^{(low)}_{j,k} )
$$

<<>>=
xx <- xtabs(~db040+hsize6+c.age+rb090+pb220a, data=rec.A)
xy <- xtabs(~db040+hsize6+c.age+rb090+pb220a+work, data=rec.A)
xz <- xtabs(~db040+hsize6+c.age+rb090+pb220a+c.netI, data=don.B)
#
library(StatMatch) #loads StatMatch
out.fbw <-  Fbwidths.by.x(tab.x=xx, tab.xy=xy, tab.xz=xz)
# average widhts of uncertainty bounds
out.fbw$av.widths
@

The results show that there is a marked reduction of the average width when passing from three to four common variables; the model with the matching variables \code{"db040"}, \code{"hsize6"}, \code{"c.age"} and \code{"rb090"} seems a good compromise among reduction of uncertainty and the necessity of not having too many matching variables.  If there is the need of having less matching variables, in this example one should consider just two variables: \code{"c.age"} and \code{"rb090"}; in fact, the models with three matching variables do not provide a great decrease of average uncertainty with respect to the ones with just two.

The choice of the matching variables is a crucial aspect.  In most of the SM applications choosing too many variables increases the complexity of the problem and may affect negatively the results of SM.  In particular, in the micro approach this may introduce additional undesired variability and bias as far as the joint (marginal) distribution of $X_M$ and $Z$ is concerned.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nonparametric micro techniques

\section{Nonparametric micro techniques} \label{sec:npmicro}

Nonparametric approach is very popular in SM when the objective is micro, i.e. the creation of a synthetic data set.  Most of the nonparametric micro approaches consists in filling in the data set chosen as the \emph{recipient} with the values of the variable which is available only in the other data set, the \emph{donor} one.  In this approach it is important to decide which data set plays the role of the recipient.  Usually this is the data set which should be used ad the basis for further statistical analysis, and a natural choice seems that of using the larger ones because it would provide more accurate results.  Unfortunately, such a way of working may provide inaccurate SM results, especially when the sizes of the two data sources are very different.  The reason is quite simple, the larger is the recipient with respect to the donor, the more times a unit in the latter could be selected as a donor; 
In this manner, there is a high risk that the distribution of the imputed variable does not reflect the original one (estimated form the donor data set).  In the following it will be assumed that $A$ is the recipient while $B$ is the donor, being $n_A \leq n_B$ ($n_A$ and $n_B$ are the sample sizes of $A$ and $B$ respectively).  Hence the objective of SM will be that of filling in $A$ with values of $Y$ (variable available only in $B$).

In \pkg{StatMatch} the following nonparametric micro techniques are available: \emph{random hot deck}, \emph{nearest neighbor hot deck} and \emph{rank hot deck} (see Section 2.4 in D'Orazio \emph{et al.}, 2006b; Singh \emph{et al.}, 1993).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nearest neighbor distance hot deck} \label{sec:nnd}

The nearest neighbor distance hot deck techniques are implemented in the function \code{NND.hotdeck}.  This function searches in \code{data.don} the nearest neighbor of each unit in \code{data.rec} according to a distance computed on the matching variables $X_M$ specified with the argument \code{match.vars}.  By default the Manhattan (city block) distance is considered (\code{dist.fun="Manhattan"}).  In order to reduce the computation effort due to computation of distances it is preferable to define some donation classes (argument \code{don.class}): for a record in given imputation class it will be selected a donor in the same class (the distances are computed only among units belonging to the same class).  Usually, the donation classes are defined according to one or more categorical common variables (geographic area, etc.).  In the following, a simple example of usage of \code{NND.hotdeck} is reported. 

<<>>=
group.v <- c("rb090","db040")
X.mtc <- c("hsize","age") 
out.nnd <- NND.hotdeck(data.rec=rec.A, data.don=don.B,
                         match.vars=X.mtc, don.class=group.v,
                         dist.fun="Manhattan")
@

The function \code{NND.hotdeck} does not create the synthetic data set; for each unit in $A$  the corresponding closest donor in $B$ is identified according to the chosen distance function; the recipient-donor units' identifiers  are saved in the data.frame \code{mtc.ids} stored in the output list returned by \code{NND.hotdeck}.  The output list provides also the distance between each couple recipient-donor (saved in the \code{dist.rd} component of the output list) and the number of available donors at the minimum distance for each recipient (component \code{noad}).  Note that when there are more donors at the minimum distance then one of them is picked up at random.

<<>>=
summary(out.nnd$dist.rd) # summary distances rec-don
summary(out.nnd$noad) # summary available donors at min. dist.
table(out.nnd$noad)
@

In order to derive the synthetic data set it is necessary to run the function \code{create.fused}:

<<>>=
head(out.nnd$mtc.ids)
fA.nnd.m <- create.fused(data.rec=rec.A, data.don=don.B,
                    mtc.ids=out.nnd$mtc.ids,
                    z.vars=c("netIncome","c.netI"))
head(fA.nnd.m) #first 6 obs.
@

As far as distances are concerned (argument \code{dist.fun}), all the distance functions in the package \pkg{proxy} (Meyer and Butchta, 2011) are available.  Anyway, for some particular distances it was decided to write specific \proglang{R} functions.  In particular, when dealing with continuous matching variables it is possible to use the \emph{maximum distance }($L^{\infty}$ norm) implemented in \code{maximum.dist}; this function works on the true observed values (continuous variables) or on transformed ranked values (argument \code{rank=TRUE}) as suggested in (Kovar \emph{et al.}, 1988); the transformation (ranks divided by the number of units) removes the effect of different scales and the new values are uniformly distributed in the interval $[0,1]$.  The Mahalanobis distance can be computed by using \code{mahalanobis.dist} which allows an external estimate of the covariance matrix (argument \code{vc}).  When dealing with mixed type matching variables, the \emph{Gowers's dissimilarity} (Gower, 1981) can be computed (function \code{gower.dist}): it is an average of the distances computed on the single variables according to different rules, depending on the type of the variable.  All the distances are scaled to range from 0 to 1, hence the overall distance cat take a value in $[0,1]$.  It is worth noting that when dealing with mixed types matching variables it is still possible to use the distance functions for continuous variables but \code{NND.hotdeck} transforms factors into dummies (by means of the function \code{fact2dummy}).


<<>>=
group.v <- c("rb090","db040")
X.mtc <- c("hsize","age","pb220a")
out.nnd.g <- NND.hotdeck(data.rec=rec.A, data.don=don.B, 
                          match.vars=X.mtc, don.class=group.v, 
                          dist.fun="Gower")

summary(out.nnd.g$dist.rd) # summary distances rec-don

fA.nnd.g <- create.fused(data.rec=rec.A, data.don=don.B,
                    mtc.ids=out.nnd.g$mtc.ids,
                    z.vars=c("netIncome","c.netI"))
@


By default \code{NND.hotdeck} does not pose constraints on the \dQuote{usage} of donors: a record in the donor data set can be selected many times as a donor.  The multiple usage of a donor can be avoided by resorting to a \emph{constrained hot deck} (argument \code{constrained=TRUE} in \code{NND.hotdeck}); in such a case, a donor can be used just once and all the donors are selected in order to minimize the overall matching distance.  In practice, the donors are identified by solving a traveling salesperson problem; two alternative algorithms are available the classic one (argument \code{constr.alg="lpSolve"}) (Berkelaar \emph{et al.}, 2011) and the RELAX-IV algorithm (Bertsekas and Tseng, 1994) (argument \code{constr.alg="relax"}).  This latter one is much faster but there are some restrictions on its license. 

<<>>=
group.v <- c("rb090","db040")
X.mtc <- c("hsize","age")
out.nnd.mc <- NND.hotdeck(data.rec=rec.A, data.don=don.B, 
                           match.vars=X.mtc, don.class=group.v, 
                           dist.fun="Manhattan", constrained=T, 
                           constr.alg="lpSolve")
fA.nnd.mc <- create.fused(data.rec=rec.A, data.don=don.B,
                    mtc.ids=out.nnd.mc$mtc.ids,
                    z.vars=c("netIncome","c.netI"))
@

The constrained matching requires a higher computational effort but preserves better the marginal distribution of the variable imputed in the synthetic data set.  Obviously the overall matching distance tends to be greater than the one in the unconstrained case.

<<>>=
# comparing marginal distribution of C.netI
tt.0 <- prop.table(xtabs(~c.netI, data=don.B))
tt.m <- prop.table(xtabs(~c.netI, data=fA.nnd.m))
tt.g <- prop.table(xtabs(~c.netI, data=fA.nnd.g))
tt.mc <- prop.table(xtabs(~c.netI, data=fA.nnd.mc))

tt <- cbind(origin=c(tt.0), m4.unc=c(tt.m), 
             g5.unc=c(tt.g), m4.constr=c(tt.mc))
round(tt*100, 2)

# distance wrt to the origin distr.
1/2*colSums(abs(tt-tt[,1]))

#overall matching distances
sum(out.nnd$dist.rd)  #unconstrained
sum(out.nnd.mc$dist.rd)  #constrained
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random hot deck} \label{sec:rand}

The function \code{RANDwNND.hotdeck} carries out the random selection of each donor from a suitable subset of all the available donors.  This subset can be formed in different ways, e.g. by considering all the donors sharing the same characteristics of the recipient (defined according to some $X_M$ variables, such as geographic region, etc.) or simply the closest donors according to a particular rule.  The traditional \emph{random hot deck} (Singh \emph{et al.}, 1993) within imputation classes is performed by simply specifying the donation classes via the argument \code{don.class} (the classes are formed by crossing the categories of the categorical variables being considered).  For each record in the recipient data set in a given donation class, a donor is picked up completely at random within the same donation class.

<<>>=
group.v <- c("db040","rb090")
rnd.1 <- RANDwNND.hotdeck(data.rec=rec.A, data.don=don.B, 
                           match.vars=NULL, don.class=group.v)
fA.rnd <- create.fused(data.rec=rec.A, data.don=don.B,
                        mtc.ids=rnd.1$mtc.ids, 
                        z.vars=c("netIncome", "c.netI"))
@

As for \code{NND.hotdeck}, the function \code{RANDwNND.hotdeck} does not create the synthetic data set; the recipient-donor units' identifiers are saved in the component \code{mtc.ids} of the list returned in output by \code{NND.hotdeck}.  The number of donors available in each donation class are saved in the component \code{noad}.

It is worth noting that the donors can also be selected with probability proportional to a weight (specified with the argument \code{weight.don}); an example will be provided in Section \ref{sec:comp_survey}. 

\code{RANDwNND.hotdeck} implements various alternative methods to restrict the subset of the potential donors.  These methods are based essentially on a distance measure computed on the matching variables provided via the argument \code{match.vars}.  In practice, when \code{cut.don="k.dist"} only the donors whose distance from the recipient is less or equal to threshold \code{k} are considered (see Andridge and Little, 2010).  By setting \code{cut.don="exact"} the \code{k} $(0 < k \leq n_D)$ closest donors are retained ($n_D$ is the number of available donors for a given recipient).  With \code{cut.don="span"} a proportion \code{k} $(0 < k \leq 1)$ of the closest available donors it is considered while with \code{cut.don="rot"} only the subset reduces to the $\left[ \sqrt{n_D}\right]$ closest donors.  Finally, when \code{cut.don="min"} only the donors at the minimum distance from the recipient are retained.


<<>>=
# random choiches of a donor among the closest k=20 wrt age
group.v <- c("db040","rb090")
X.mtc <- "age"
rnd.2 <- RANDwNND.hotdeck(data.rec=rec.A, data.don=don.B, 
                           match.vars=X.mtc, don.class=group.v, 
                           dist.fun="Manhattan", 
                            cut.don="exact", k=20)
fA.knnd <- create.fused(data.rec=rec.A, data.don=don.B,
                        mtc.ids=rnd.2$mtc.ids, 
                        z.vars=c("netIncome", "c.netI"))
@

When distances are computed on some matching variables, then the output of \code{RANDwNND.hotdeck} provides some information concerning the distances of the possible available donors for each recipient observation.

<<>>=
head(rnd.2$sum.dist)
@

In particular, \code{"min"}, \code{"max"} and \code{"sd"} columns report respectively the minimum, the maximum and the standard deviation of the distances (all the available donors are considered), while \code{"cut"} refers to the distance of the \code{k}th closest donor; \code{"dist.rd"} is distance existing among the recipient and the randomly chosen donor.

<<>>=
tt.0 <- prop.table(xtabs(~c.netI, data=don.B))
tt.rnd <- prop.table(xtabs(~c.netI, data=fA.rnd))
tt.knnd <- prop.table(xtabs(~c.netI, data=fA.knnd))

tt <- cbind(origin=c(tt.0), rnd=c(tt.rnd), k.nnd=c(tt.knnd))
round(tt*100, 2)

# distance wrt to the origin distr.
1/2*colSums(abs(tt-tt[,1]))
@

When selecting a donor among those available in the subset identified by \code{cut.don} and \code{k} it is possible to use a weighted selection by specifying a weighting variable via \code{weight.don} argument.  The aspects related to the usage of weights will be covered in Section \ref{sec:comp_survey}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Rank hot deck} \label{sec:rank}

The \emph{rank hot deck distance} method has been introduced by (Singh \emph{et al.}, 1993).  It searches for the donor at a minimum distance from the given recipient record but, in this case, the distance is computed on the percentage points of the empirical cumulative distribution function of the unique (continuous) common variable $X_M$ being considered.  The empirical cumulative distribution function is estimated by:

$$
\hat{F}(x_k) = \frac{1}{n} \sum_{i=1}^{n} I\left(x_i\leq x_k\right) 
$$

\noindent being $I()=1$ if $x_i\leq x_k$ and 0 otherwise.  This transformation provides values uniformly distributed in the interval $\left[0,1\right]$; moreover, it can be useful when the values of $X_M$ can not be directly compared because of measurement errors which however do not affect the \dQuote{position} of a unit in the whole distribution (D'Orazio \emph{et al.}, 2006b).  This method is implemented in the function \code{rankNND.hotdeck}.  In the following just a simple example of usage is reported.

<<>>=
rnk.1 <- rankNND.hotdeck(data.rec=rec.A, data.don=don.B, 
                          var.rec="age", var.don="age")
#create the synthetic data set
fA.rnk <- create.fused(data.rec=rec.A, data.don=don.B,
                        mtc.ids=rnk.1$mtc.ids, 
                        z.vars=c("netIncome", "c.netI"), 
                        dup.x=TRUE, match.vars="age")
head(fA.rnk)
@

The function \code{rankNND.hotdeck} allows defining some donation classes with the argument \code{don.class}; in this case the empirical cumulative distribution is estimated separately class by class.

<<>>=
rnk.2 <- rankNND.hotdeck(data.rec=rec.A, data.don=don.B, var.rec="age",
                        var.don="age", don.class="db040")
fA.grnk <- create.fused(data.rec=rec.A, data.don=don.B,
                        mtc.ids=rnk.2$mtc.ids, 
                        z.vars=c("netIncome", "c.netI"),
                        dup.x=TRUE, match.vars="age")
head(fA.grnk)
@

In estimating the empirical cumulative distribution it is possible to consider the weights of the observations (arguments \code{weight.rec} and \code{weight.don}).  This topic will be covered in Section \ref{sec:comp_survey}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% imputation of missing data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Using functions in \pkg{StatMatch} to impute missing values in a survey} \label{sec:imp}

All the functions in \pkg{StatMatch} that implement the hot deck imputation techniques can be used to impute missing values in a single data set.  In this case it is necessary to separate the observations in two data sets: the file $A$ plays the role of recipient and will contain the units with missing values on the target variable while the file $B$ is the donor and will contain all the available donors (units with non missing values for the target variable).  In the following a simple example with the \code{iris} data.frame is reported.

<<>>=
# introduce missing values in iris
set.seed(1324)
miss <- rbinom(150, 1, 0.30) #generates randomly missing
data(iris, package="datasets")
iris.miss <- iris
iris.miss$Petal.Length[miss==1] <- NA
summary(iris.miss$Petal.L)

# separate units in two data sets
rec <- subset(iris.miss, is.na(Petal.Length), select=-Petal.Length)
don <- subset(iris.miss, !is.na(Petal.Length))
@

Once the starting data set has been split in two (recipient and donor) a nonparametric imputation technique can be used to search for a donor for each observation in the recipient data set.  In the following example the nearest neighbor hotdeck is considered.

<<>>=
# search for closest donors
X.mtc <- c("Sepal.Length", "Sepal.Width", "Petal.Width")
nnd <- NND.hotdeck(data.rec=rec, data.don=don,
                         match.vars=X.mtc, don.class="Species",
                         dist.fun="Manhattan")
@

At this point the function \code{create.fused} is used to impute the target variable (\code{"Petal.Length"} in the example) then the filled recipient and the donor one can be re-aggregated to obtain the origin dataset with imputed values.

<<>>=
# fills rec
imp.rec <- create.fused(data.rec=rec, data.don=don,
                        mtc.ids=nnd$mtc.ids, z.vars="Petal.Length")
imp.rec$imp.PL <- 1 # flag for imputed
#
# re-aggregate data sets
don$imp.PL <- 0
imp.iris <- rbind(imp.rec, don)
summary(imp.iris)
#summary stat of imputed and non imputed Petal.Length
tapply(imp.iris$Petal.Length, imp.iris$imp.PL, summary)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mixed methods} \label{sec:mix}

A SM mixed method consists of two steps: (1) a model is fitted and all its parameters are estimated, then (2) a nonparametric approach is used to create the synthetic data set.  The model is more parsimonious while the nonparametric approach offers \dQuote{protection} against model misspecification.  The proposed mixed approaches for SM are based essentially on \emph{predictive mean matching} imputation methods (see D'Orazio \emph{et al.} 2006b, Section 2.5 and 3.6).  The function \code{mixed.mtc} in \pkg{StatMatch} implements two similar mixed methods that deal with variables $(X_M, Y, Z)$ following the the multivariate normal distribution.  The main difference consists in the estimation of the parameters of the two regressions $Y$ vs. $X_M$  and $Z$ vs. $X_M$.  By default the parameters are estimated through maximum likelihood (argument \code{method="ML"}); in alternative a method proposed by Moriarity and Scheuren (2001, 2003) (argument \code{method="MS"}) is available.  D'Orazio \emph{et al.} (2005) compared these methods in an extensive simulation study: in general ML tends to perform better, moreover it permits to avoid some incoherencies in the estimation of the parameters that can happen with the Moriarity and Scheuren approach.

At the end of the first step, after the estimation of the parameters of the two regression models, the data set $A$ is filled in with the \dQuote{intermediate} values $\tilde{z}_a=\hat{z}_a+e_a$ $(a=1,\ldots,n_A)$ obtained by adding a random residual term $e_a$ to the predicted values $\hat{z}_a$.  The same happens in $B$ which is filled in with the values $\tilde{y}_b=\hat{y}_b+e_b$ 
$(b=1,\ldots, n_B)$.  Finally, in the step (2) each record in $A$ is filled in with the value of $Z$ observed on the donor found in $B$ according to a constrained distance hot deck; the Mahalanobis distance is computed by considering the intermediate and live values: couples $\left( y_a,\tilde{z}_a \right)$ in $A$ and $\left( \tilde{y}_b, z_b \right)$ in $B$.

Such a two steps procedure offers various advantages: it offers protection against model misspecification and also reduces the risk of bias in the marginal distribution of the imputed variable because the distances are computed on intermediate and truly observed values of the target value instead of the matching variables $X_M$.  In fact when computing the distances by considering all the matching variables, variables with low predictive power on the target variable may influence negatively the distances.

In the following example the \code{iris} data set is used just to show how \code{mixed.mtc} works.

<<>>=
# uses iris data set
iris.A <- iris[101:150, 1:3]
iris.B <- iris[1:100, c(1:2,4)]

X.mtc <- c("Sepal.Length","Sepal.Width") # matching variables

# parameters estimated using ML
mix.1 <- mixed.mtc(data.rec=iris.A, data.don=iris.B, match.vars=X.mtc,
                    y.rec="Petal.Length", z.don="Petal.Width", 
                    method="ML", rho.yz=0, 
                    micro=TRUE, constr.alg="lpSolve")

mix.1$mu #estimated means
mix.1$cor #estimated cor. matrix

head(mix.1$filled.rec) # A filled in with Z
cor(mix.1$filled.rec)
@

When using \code{mixed.mtc} the synthetic data set is provided in output as the component \code{filled.rec} of the list returned by calling it with the argument \code{micro=TRUE}.  When \code{micro=FALSE} the function \code{mixed.mtc} returns just the estimates of the parameters (parametric macro approach).

The function \code{mixed.mtc} by default performs mixed SM under the CI assumption ($\rho_{YZ|X_M}=0$ argument \code{rho.yz=0}).  When some additional auxiliary information about the correlation between $Y$ and $Z$ is available (estimates from previous surveys or form external sources) then it can be exploited in SM by specifying a value $(\neq0)$ for the argument \code{rho.yz}; it represents guess for $\rho_{YZ|X_M}$ when using the ML estimation or a guess for $\rho_{YZ}$ when estimating the parameters by using the Moriarity and Scheuren method.

<<>>=
# parameters estimated using ML and rho_YZ|X=0.85
mix.2 <- mixed.mtc(data.rec=iris.A, data.don=iris.B, match.vars=X.mtc,
                    y.rec="Petal.Length", z.don="Petal.Width", 
                    method="ML", rho.yz=0.85, 
                    micro=TRUE, constr.alg="lpSolve")
mix.2$cor
head(mix.2$filled.rec)
@

Special attention is required when specifying a guess for $\rho_{YZ}$ under the Moriarity and Scheuren estimation approach (\code{method="MS"}); in particular it may happen that the specified value for $\rho_{YZ}$ is not compatible with the given SM framework (remember that the correlation matrix must be positive semidefinite).  If this is the case, then the \code{mixed.mtc} substitutes the input value \code{rho.yz} by its closest admissible value, as shown in the following example.

<<>>=
mix.3 <- mixed.mtc(data.rec=iris.A, data.don=iris.B, match.vars=X.mtc,
                    y.rec="Petal.Length", z.don="Petal.Width", 
                    method="MS", rho.yz=0.75, 
                    micro=TRUE, constr.alg="lpSolve")

mix.3$rho.yz
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Statistical matching with data from complex surveys
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical matching of data from complex sample surveys} \label{sec:comp_survey}

The SM techniques presented in the previous Sections implicitly or explicitly assume that the observed values in $A$ and $B$ are i.i.d. Unfortunately, when dealing with samples selected from a finite population by means of complex sampling designs (with stratification, clustering, etc.) it is difficult to maintain the i.i.d. assumption (it would mean that the sampling design can be ignored) and the sampling design and the weights assigned to the units (usually design weights corrected for unit nonresponse, frame errors, etc.) have to be considered when making inferences (see S\"arndal \emph{et al.}, 1992, Section 13.6). 

\subsection{Naive micro approaches} \label{sec:comp_survey_naive}

A naive approach to SM data from complex sample surveys consists in applying SM nonparametric micro methods (NND, random or rank hotdeck) without considering the design nor the units weights.  Once obtained the synthetic dataset (recipient filled in with the missing variables) the successive statistical analyses are carried out by considering the sampling design underlying the recipient data set and the corresponding survey weights.  In the following a simple example of nearest neighbor hotdeck is reported.

<<>>=
# summary info on the weights
sum(rec.A$wwA) # estimated pop size from A
sum(don.B$wwB) # estimated pop size from B
summary(rec.A$wwA)
summary(don.B$wwB)

# NND unconstrained hotdeck
group.v <- c("rb090","db040")
X.mtc <- c("hsize","age")
out.nnd <- NND.hotdeck(data.rec=rec.A, data.don=don.B,
                         match.vars=X.mtc, don.class=group.v,
                         dist.fun="Manhattan")

fA.nnd.m <- create.fused(data.rec=rec.A, data.don=don.B,
                    mtc.ids=out.nnd$mtc.ids,
                    z.vars=c("netIncome","c.netI"))

# estimating average net income
weighted.mean(fA.nnd.m$netIncome, fA.nnd.m$wwA) # imputed in A
weighted.mean(don.B$netIncome, don.B$wwB) # ref. estimate in B

# comparing marginal distribution of C.netI using weights
tt.0w <- prop.table(xtabs(wwB~c.netI, data=don.B))
tt.fw <- prop.table(xtabs(wwA~c.netI, data=fA.nnd.m))

tt <- cbind(origin=c(tt.0w), nnd.naive=c(tt.fw))
round(tt*100, 2)

# distance wrt to the origin distr.
1/2*colSums(abs(tt-tt[,1]))
@

As far as imputation of missing values is concerned, a way of taking into account the sampling design can consist in forming the donation classes by using the design variables (stratification and/or clustering variables) jointly with the most relevant common variables (Andridge and Little, 2010).  Unfortunately in SM this can increase the complexity or may be unfeasible because the design variables may not be available or may be partly available.  Moreover, the two sample surveys may have quite different designs and the design variables used in one survey maybe not available in the other one and vice versa. 

When imputing missing values in a survey, another possibility, consists in using sampling weights (design weights) to form the donation classes (Andridge and Little, 2010).  But again, in SM applications the problem can be slightly more complex even because the sets of weights can be quite different from one survey to the other (usually the available weights are the design weights corrected to compensate for unit nonresponse, to satisfy some given constraints etc.).  The same Authors (Andridge and Little, 2010) indicate that when imputing the missing values, the selection of the donors can be carried out with probability proportional to weights associated to the donors (\emph{weighted random hot deck}).  Unfortunately, Andridge and Little (2009) found that such a usage of the the sample weights can give poor results.  The properties of the weighted random hot deck in the SM applications have not been investigated, nevertheless the function \code{RANDwNDD.hotdeck} permits to select the donors with probability proportional to weights specified via the \code{weight.don} argument.

<<>>=

rnd.2 <- RANDwNND.hotdeck(data.rec=rec.A, data.don=don.B, 
                           match.vars=NULL, don.class=group.v, 
                           weight.don="wwB")
fA.wrnd <- create.fused(data.rec=rec.A, data.don=don.B, 
                         mtc.ids=rnd.2$mtc.ids,
                         z.vars=c("netIncome","c.netI"))
head(fA.wrnd)
weighted.mean(fA.wrnd$netIncome, fA.wrnd$wwA) # imputed in A
weighted.mean(don.B$netIncome, don.B$wwB) # ref. estimate in B

# comparing marginal distribution of C.netI using weights
tt.0w <- prop.table(xtabs(wwB~c.netI, data=don.B))
tt.fw <- prop.table(xtabs(wwA~c.netI, data=fA.wrnd))

tt <- cbind(origin=c(tt.0w), nnd.naive=c(tt.fw))
round(tt*100, 2)

# distance wrt to the origin distr.
1/2*colSums(abs(tt-tt[,1]))
@

Perhaps, a better usage of the units weights is achieved in the rank hot deck SM where the the weights of the units $w_i$ can be used  in estimating the percentage points of the the empirical cumulative distribution function:

$$
\hat{F}(x_k) = \frac{\sum_{i=1}^n w_i I\left(x_i \leq x_k \right)}{\sum_{i=1}^n w_i}
$$

Such a procedure can provide quite good results in terms of preservation of the marginal distribution of the imputed variables in the synthetic data set.  In the following it is reported an very simple example.

<<>>=

rnk.w <- rankNND.hotdeck(data.rec=rec.A, data.don=don.B, 
                          don.class="db040", var.rec="age", 
                          var.don="age", weight.rec="wwA",
                           weight.don="wwB")
#
#create the synthetic data set
fA.wrnk <- create.fused(data.rec=rec.A, data.don=don.B,
                        mtc.ids=rnk.w$mtc.ids, 
                        z.vars=c("netIncome", "c.netI"), 
                        dup.x=TRUE, match.vars="age")
#
weighted.mean(fA.wrnk$netIncome, fA.wrnk$wwA) # imputed in A
weighted.mean(don.B$netIncome, don.B$wwB) # ref. estimate in B

# comparing marginal distribution of C.netI using weights
tt.0w <- prop.table(xtabs(wwB~c.netI, data=don.B))
tt.fw <- prop.table(xtabs(wwA~c.netI, data=fA.wrnk))

tt <- cbind(origin=c(tt.0w), nnd.naive=c(tt.fw))
round(tt*100, 2)

# distance wrt to the origin distr.
1/2*colSums(abs(tt-tt[,1]))
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistical matching method that account explicitly for the sampling weights} \label{sec:ren}

In literature there are few SM methods that explicitly take into account the sampling design and the corresponding sampling weights: Renssen's approach based on weights' \emph{calibrations} (Renssen, 1998); Rubin's \emph{file concatenation} (Rubin, 1986) and the approach based on the empirical likelihood proposed by Wu (2004).  A comparison among these approaches can be found in D'Orazio (2010).

The package \pkg{StatMatch} provides only functions to apply the procedures suggested by Renssen (1998).  This approach consists in a series of calibration steps of the survey weights of $A$ and $B$ in order to achieve consistency between estimates (mainly totals) computed separately from them.     Calibration is a technique very common in sample surveys for deriving new survey weights, as close as possible to the starting ones, which fulfill a series of constraints concerning totals for a set of auxiliary variables (for further details on calibration see S\"arndal, 2005).  The Renssen's approach works well when dealing with categorical variables or in a mixed case in which the number of continuous variables is very limited.  In the following it will be assumed that all the variables $(X_M,Y,Z)$ are categorical.  The procedure and the functions developed in \pkg{StatMatch} permit to have one or more continuous variables (better just one) in the subset of the matching variables $X_M$, while $Y$ and $Z$ are assumed to be categorical.  Obviously when this is not the case in order to apply the following procedure it is necessary to categorize the variables.

The first step in the Renssen's procedure consists in calibrating weights in $A$ and in $B$ such that the new weights when applied to the set of the matching variables $X_M$ allow to reproduce some known (or estimated) population totals.  In \pkg{StatMatch} the harmonization step can be performed by using \code{harmonize.x}.  This function performs weights calibration (or post-stratification) by means of functions available in the \proglang{R} package \pkg{survey} (Lumley, 2011).  When the population totals are already known then they have to be passed to \code{harmonize.x} via the argument \code{x.tot}; on the contrary, when they are unknown (\code{x.tot=NULL}) they are estimated by a weighted average of the totals estimated on the two surveys before the harmonization step:

$$
\tilde{t}_{X_M} = \lambda \hat{t}_{X_M}^{(A)} + \left( 1-\lambda \right) \hat{t}_{X_M}^{(B)}
$$

\noindent being $\lambda= n_A / (n_A+n_B) $ $(n_A$ and $n_B$ are the sample sizes of $A$ and $B$ respectively) (Korn and Graubard, 1999, pp. 281--284).

The following example shows how to harmonize the joint distribution of the gender and classes of age with the data from the previous example, assuming that the joint distribution of age and gender is not known.

<<>>=
tt.A <- xtabs(wwA~rb090+c.age, data=rec.A)
tt.B <- xtabs(wwB~rb090+c.age, data=don.B)
(prop.table(tt.A)-prop.table(tt.B))*100

library(survey) # loads survey
# creates svydesign objects
svy.rec.A <- svydesign(~1, weights=~wwA, data=rec.A)
svy.don.B <- svydesign(~1, weights=~wwB, data=don.B)
#
# harmonizes wrt to joint distr. of gender vs. c.age
out.hz <- harmonize.x(svy.A=svy.rec.A, svy.B=svy.don.B,
                form.x=~c.age:rb090-1)
#
summary(out.hz$weights.A) # new calibrated weights for A
summary(out.hz$weights.B) # new calibrated weights for B

tt.A <- xtabs(out.hz$weights.A~rb090+c.age, data=rec.A)
tt.B <- xtabs(out.hz$weights.B~rb090+c.age, data=don.B)
(prop.table(tt.A)-prop.table(tt.B))*100
@

After the harmonization, the second step in the Renssen's procedure consists in estimating the two way contingency table $Y \times Z$.  In absence of auxiliary information it  is estimated under the CI assumption by means of:

$$
\hat{P}^{\left( CIA\right)} \left( Y,Z \right) = \hat{P}^{(A)} \left( Y|X_M \right)  \hat{P}^{(B)} \left( Z|X_M \right) \hat{P} \left(X_M \right)
$$

In practice, $\hat{P}^{(A)} \left( Y|X_M \right)$ is computed from $A$; $\hat{P}^{(B)} \left( Z|X_M \right)$ is computed from data in $B$ while $P \left(X_M \right)$ can be estimated indifferently from $A$ or $B$ (the data set are harmonized with respect to the $X_M$ distribution).  In \pkg{StatMatch} an estimate of the table $Y \times Z$ under the CIA is provided by the function \code{comb.samples}.


<<>>=
# estimating c.pl030 vs. c.netI under the CI assumption
out <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
            svy.C=NULL, y.lab="work", z.lab="c.netI",
            form.x=~c.age:rb090-1)
#
addmargins(t(out$yz.CIA))  # table estimated under the CIA
@

When some auxiliary information represented by third data source $C$, containing all the variables $(X_M,Y,Z)$ or just $(Y,Z)$, is available Renssen's approach permits to exploit it in estimating $Y \times Z$.  Two alternative methods are available: (a) \emph{incomplete two way stratification}; and (b) \emph{synthetic two way stratification}.  In practice, both the methods estimate $Y \times Z$ from $C$ after some further calibration steps (for further details see Renssen, 1998).  The function \code{comb.samples} implements both the methods.  In practice, the synthetic two way stratification (argument \code{estimation="synthetic"}) can be applied only when $C$ contains all the variables of interest $(X_M,Y,Z)$; on the contrary, when the data source $C$ observes just $Y$ and $Z$ only the incomplete two way stratification method can be applied (argument \code{estimation="incomplete"}).  In the following a simple example is reported based on the artificial EU-SILC data introduced in Section \ref{sec:data}; here a small sample $C$ $(n_C=200)$ with all the variables of interest $(X_M,Y,Z)$ is artificially created.

<<>>=
# generating artificial sample C
set.seed(43210)
obs.C <- sample(nrow(silc.16), 200, replace=F)
#
X.vars <- c("hsize","hsize6","db040","age","c.age",
            "rb090","pb220a", "rb050")
y.var <- c("pl030","work")
z.var <- c("netIncome","c.netI")
#
aux.C <- silc.16[obs.C, c(X.vars, y.var, z.var)]
aux.C$wwC <- aux.C$rb050/sum(aux.C$rb050)*round(sum(silc.16$rb050)) # rough w
svy.aux.C <- svydesign(~1, weights=~wwC, data=aux.C) 
#
# incomplete two-way estimation
out.inc <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
                svy.C=svy.aux.C, y.lab="work", z.lab="c.netI",
                form.x=~c.age:rb090-1, estimation="incomplete")
#
addmargins(t(out.inc$yz.est))           
new.wwC <- weights(out.inc$cal.C) #new cal. weights for C 
#
# marginal distributions of work
m.work.cA <- xtabs(out.hz$weights.A~work, data=rec.A)
m.work.cC <- xtabs(new.wwC~work, data=aux.C)
m.work.cA-m.work.cC
#
# marginal distributions of c.netI
m.cnetI.cB <- xtabs(out.hz$weights.B~c.netI, data=don.B)
m.cnetI.cC <- xtabs(new.wwC~c.netI, data=aux.C)
m.cnetI.cB-m.cnetI.cC 

# joint distribution of the matching variables
tt.A <- xtabs(out.hz$weights.A~rb090+c.age, data=rec.A)
tt.B <- xtabs(out.hz$weights.B~rb090+c.age, data=don.B)
tt.C <- xtabs(new.wwC~rb090+c.age, data=aux.C)
(prop.table(tt.A)-prop.table(tt.B))*100 
(prop.table(tt.A)-prop.table(tt.C))*100  

#distance tt.A-tt.C
1/2*sum(abs(prop.table(tt.A)-prop.table(tt.C)))
@

The incomplete two way stratification method estimates the table $Y \times Z$ from $C$ by preserving the marginal distribution of $Y$ and of $Z$ estimated respectively from $A$ and from $B$ after the initial harmonization step; on the contrary, the joint distribution of the matching variables (which is the basis of the harmonization step) is not preserved. 

<<>>=
# synthetic two-way estimation
out.synt <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
                svy.C=svy.aux.C, y.lab="work", z.lab="c.netI",
                form.x=~c.age:rb090-1, estimation="synthetic")
#
addmargins(t(out.synt$yz.est))           
new.wwC <- weights(out.synt$cal.C) #new cal. weights for C 
#
# marginal distributions of work
m.work.cA <- xtabs(out.hz$weights.A~work, data=rec.A)
m.work.cC <- xtabs(new.wwC~work, data=aux.C)
m.work.cA-m.work.cC

# marginal distributions of c.netI
m.cnetI.cB <- xtabs(out.hz$weights.B~c.netI, data=don.B)
m.cnetI.cC <- xtabs(new.wwC~c.netI, data=aux.C)
m.cnetI.cB-m.cnetI.cC 

# joint distribution of the matching variables
tt.A <- xtabs(out.hz$weights.A~rb090+c.age, data=rec.A)
tt.B <- xtabs(out.hz$weights.B~rb090+c.age, data=don.B)
tt.C <- xtabs(new.wwC~rb090+c.age, data=aux.C)
(prop.table(tt.A)-prop.table(tt.B))*100 
(prop.table(tt.A)-prop.table(tt.C))*100  
#distance tt.A-tt.C
1/2*sum(abs(prop.table(tt.A)-prop.table(tt.C)))
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% uncertainty

\section{Exploring uncertainty due to the statistical matching framework} \label{sec:unc}

When the objective of SM consists in estimating a parameter (macro approach) it is possible to tackle SM in an alternative way consisting in the \dQuote{exploration} of the uncertainty on the model chosen for $(X_M,Y,Z)$, due to the lack of knowledge typical of the basic SM framework (no auxiliary information is available).  This approach does not end with a unique estimate of the unknown parameter characterizing the joint p.d.f. for $(X_M,Y,Z)$; on the contrary it identifies an interval of plausible values for it.  When dealing with categorical variables, the estimation of the intervals of plausible values for the probabilities in the table $Y \times Z$ are provided by the Fr\'echet bounds:

$$
\max\{0; P(Y=j) + P(Z=k) - 1\} \leq P(Y=j,Z=k) \leq \min \{P(Y=j); P(Z=k)\}
$$

\noindent for $j=1,\ldots, J$ and $k=1,\ldots, K$, being $J$ and $K$ the categories of $Y$ and $Z$ respectively.

If the $X_M$ variables are introduced, by conditioning on them, it is possible to derive the following result (D'Orazio \emph{et al.}, 2006a):

$$
P^{(low)}_{j,k} \leq P(Y=j,Z=k) \leq P^{(up)}_{j,k}
$$

with

\begin{eqnarray}
P^{(low)}_{j,k} &=& \sum_{i}  P(X_M=i) \max \left\{ 0; P(Y=j|X_M=i) + P(Z=k|X_M=i)-1 \right\} \nonumber\\
P^{(up)}_{j,k} &=& \sum_{i}  P(X_M=i) \min \left\{ P(Y=j|X_M=i); P(Z=k|X_M=i) \right\} \nonumber
\end{eqnarray}

\noindent for $j=1,\ldots, J$ and $k=1,\ldots, K$.

In the SM basic framework, the probabilities $P(Y=j|X_M=i)$ are estimated from $A$, the $P(Z=k|X_M=i)$ are estimated from $B$ while the marginal distribution $P(X_M=i)$ can be estimated indifferently on $A$ or on $B$, assuming that both the samples, being representative samples of the same population, provide not significantly different estimates of $P(X_M=i)$.  If this is not the case, before computing the bounds it would be preferable to harmonize the distribution of the $X_M$ variables in $A$ and in $B$ by using the function \code{harmonize.x}.

In \pkg{StatMatch} the Fr\'echet bounds for $P(Y=j,Z=k)$ ($j=1,\ldots, J$ and $k=1,\ldots, K$), conditioning or not on the $X_M$ variables, are provided by \code{Frechet.bounds.cat}.

<<>>=
#comparing joint distribution of the X_M variables in A and in B
t.xA <- xtabs(wwA~c.age+rb090, data=rec.A)
t.xB <- xtabs(wwB~c.age+rb090, data=don.B)
prop.table(t.xA)-prop.table(t.xB)
#
#computing tables needed by Frechet.bounds.cat
t.xy <- xtabs(wwA~c.age+rb090+work, data=rec.A)
t.xz <- xtabs(wwB~c.age+rb090+c.netI, data=don.B)
out.fb <- Frechet.bounds.cat(tab.x=t.xA, tab.xy=t.xy, tab.xz=t.xz, 
                             print.f="data.frame")
out.fb
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Acknowledgments}
%This work was developed in the framework of the ESSnet project on Data Integration partly funded by the Eurostat (December 2009--December 2011).  For more information on the project visit \url{http://www.essnet-portal.eu/di/data-integration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ------------
% bibliography
% ------------

\section*{References} \label{ref} 

\small
\begin{flushleft}

\begin{description}
 \setlength{\itemsep}{0pt}

\item[] Alfons A., Kraft S. (2011) \dQuote{simPopulation: Simulation of synthetic populations for surveys based on sample data}. R package version 0.3. \url{http://CRAN.R-project.org/package=simPopulation}

\item[] Andridge R.R., Little R.J.A. (2009) \dQuote{The Use of Sample Weights in Hot Deck Imputation}. \emph{Journal of Official Statistics}, \textbf{25}(1), 21--36.

\item[] Andridge R.R., Little R.J.A. (2010) \dQuote{A Review of Hot Deck Imputation for Survey Nonresponse}. \emph{International Statistical Review}, \textbf{78}, 40--64.

\item[] Berkelaar M. and others (2011) \dQuote{lpSolve: Interface to Lpsolve v. 5.5 to solve linear--integer programs}. R package version 5.6.6. \url{http://CRAN.R-project.org/package=lpSolve}
  
\item[] Bertsekas D.P., Tseng P. (1994). \dQuote{RELAX-IV: A Faster Version of the RELAX Code for Solving Minimum Cost Flow Problems}. \emph{Technical Report}, LIDS-P-2276, Massachusetts Institute of Technology, Cambridge. \url{http://web.mit.edu/dimitrib/www/RELAX4_doc.pdf}

\item[] Breiman, L. (2001) \dQuote{Random Forests}, \emph{Machine Learning}, \textbf{45}(1), 5--32.

\item[] Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) \emph{Classification and Regression Trees}. Wadsworth. 

\item[] Cohen M. L. (1991) \dQuote{Statistical matching and microsimulation models}, in Citro and Hanushek (eds) \emph{Improving Information for Social Policy Decisions: The Uses of Microsimulation Modeling. Vol II Technical papers}. Washington D.C.

\item[] D'Orazio M. (2010) \dQuote{Statistical matching when dealing with data from complex survey sampling}, in \emph{Report of WP1. State of the art on statistical methodologies for data integration}, ESSnet project on Data Integration, 33--37, \url{http://www.essnet-portal.eu/sites/default/files/131/ESSnetDI_WP1_v1.32.pdf}

\item[] D'Orazio M. (2011) \dQuote{StatMatch: Statistical Matching}. R package version 1.0.3. 
\url{http://CRAN.R-project.org/package=StatMatch}

\item[] D'Orazio M., Di Zio M., Scanu, M. (2005) \dQuote{A comparison among different estimators of regression parameters on statistically matched files trough an extensive simulation study}. \emph{Contributi Istat}, \textbf{2005/10}

\item[] D'Orazio M., Di Zio M., Scanu M. (2006a) \dQuote{Statistical matching for categorical data: Displaying uncertainty and using logical constraints}. \emph{Journal of Official Statistics } {\textbf 22}, 137--157.

\item[] D'Orazio M., Di Zio M., Scanu M. (2006b) \emph{Statistical matching: Theory and practice}. Wiley, Chichester.

\item[] D'Orazio M., Di Zio M., Scanu M. (2008) \dQuote{The statistical matching workflow}, in: \emph{Report of WP1: State of the art on statistical methodologies for integration of surveys and administrative data}, \dQuote{ESSnet Statistical Methodology Project on Integration of Survey and Administrative Data}, 25--26. \url{http://cenex-isad.istat.it/}

\item[] D'Orazio M., Di Zio M., Scanu M. (2010) \dQuote{Old and new approaches in statistical matching when samples are drawn with complex survey designs}. \emph{Proceedings of the 45th \dQuote{Riunione Scientifica della Societa' Italiana di Statistica}}, Padova 16--18 June 2010.

\item[] Gower J. C. (1971) \dQuote{A general coefficient of similarity and some of its properties}. \emph{Biometrics}, \textbf{27}, 623--637.

\item[] Hothorn T., Hornik K., Zeileis A. (2006) \dQuote{Unbiased Recursive Partitioning: A Conditional
  Inference Framework}. \emph{Journal of Computational and Graphical Statistics}, \textbf{15}(3), 651--674.

\item[] Korn E.L., Graubard B.I. (1999) \emph{Analysis of Health Surveys}. Wiley, New York

\item[] Kovar J.G., MacMillan J., Whitridge P. (1988) \dQuote{Overview and strategy for the Generalized Edit and Imputation System}. Statistics Canada, Methodology Working Paper, No. BSMD 88-007 E/F.

\item[] Liaw A., Wiener M. (2002) \dQuote{Classification and Regression by randomForest}. \emph{R News}, \textbf{2}(3), 18--22.

\item[] Lumley T. (2011) \dQuote{survey: analysis of complex survey samples}. R package version 3.24-1. \url{http://CRAN.R-project.org/package=survey}

\item[] Meyer D., Buchta C. (2011) \dQuote{proxy: Distance and Similarity Measures}. R package version 0.4-7.  \url{http://CRAN.R-project.org/package=proxy}

\item[] Moriarity C., Scheuren F. (2001) \dQuote{Statistical matching: a paradigm for assessing the uncertainty in the procedure}. \emph{Journal of Official Statistics}, \textbf{17}, 407--422. 

\item[] Moriarity C., Scheuren F. (2003). \dQuote{A note on Rubin's statistical matching using file concatenation with adjusted weights and multiple imputation}, \emph{Jour. of Business and Economic Statistics}, \textbf{21}, 65--73.

\item[] R Development Core Team (2011) \emph{R: A language and environment for statistical computing. R Foundation for Statistical Computing}, Vienna, Austria. ISBN 3-900051-07-0, \url{http://www.R-project.org/}.

\item[] R\"assler S. (2002) \emph{Statistical matching: a frequentist theory, practical applications and alternative Bayesian approaches}. Springer Verlag, New York.

\item[] Renssen R.H.(1998) \dQuote{Use of statistical matching techniques in calibration estimation}. \emph{Survey Methodology} {\textbf 24}, 171--183.

\item[] Rubin D.B. (1986) \dQuote{Statistical matching using file concatenation with adjusted weights and multiple imputations}. \emph{Journal of Business and Economic Statistics}, {\textbf 4}, 87--94.

\item[] S\"arndal C.E., Swensson B., Wretman J. (1992) \emph{Model Assisted Survey Sampling}. Springer-Verlag, New York.

\item[] S\"arndal C.E., Lundstr\"om S. (2005) \emph{Estimation in Surveys with Nonresponse}. Wiley, New York.

\item[] Scanu M. (2008) \dQuote{The practical aspects to be considered for statistical matching}. in: \emph{Report of WP2: Recommendations on the use of methodologies for the integration of surveys and administrative data}, \dQuote{ESSnet Statistical Methodology Project on Integration of Survey and Administrative Data}, 34--35. \url{http://cenex-isad.istat.it/}

\item[] Singh A.C., Mantel H., Kinack M., Rowe G. (1993) \dQuote{Statistical matching: use of auxiliary information as an alternative to the conditional independence assumption}. \emph{Survey Methodology}, \textbf{19}, 59--79.

\item[] Wu C. (2004) \dQuote{Combining information from multiple surveys through the empirical likelihood method}. \emph{The Canadian Journal of Statistics}, \textbf{32}, 15--26.

\end{description}
\end{flushleft}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
